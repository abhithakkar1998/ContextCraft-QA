{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49df159f-4af0-4651-ab44-c69c3aa40731",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7a3e4451-ab57-4a47-89e9-6207d340c9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import json\n",
    "import faiss\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505d8d88-1371-44d1-b30c-e546164fe7f8",
   "metadata": {},
   "source": [
    "#### Chunking the documents\n",
    "\n",
    "Chunking is necessary to handle the input length limitations of language models and to improve the relevance of the retrieved context.\n",
    "\n",
    "A simple approach is to use fixed-size chunking with some overlap:\n",
    "* Fixed Size: We decide on a maximum number of characters (or tokens, though character-based is simpler to start with) for each chunk. For example, we could aim for chunks of around 500-1000 characters.\n",
    "* Overlap: To maintain context between consecutive chunks, we can introduce an overlap. For instance, if our chunk size is 500 characters and we use an overlap of 100 characters, the end of one chunk will be the beginning of the next. This helps the language model understand the flow of information across chunks.\n",
    "\n",
    "We'll read each document we loaded earlier and split its content into chunks of a specified size with a defined overlap. For now, we can just print the chunks and their metadata to verify the chunking process is working correctly, and in the next step, we'll integrate the embedding generation and vector database storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b366c517-b890-444d-8203-6a567aba4691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk from: A.I._Artificial_Intelligence.txt\n",
      "Length: 1000\n",
      "--------------------\n",
      "Chunk from: A.I._Artificial_Intelligence.txt\n",
      "Length: 1000\n",
      "--------------------\n",
      "Chunk from: A.I._Artificial_Intelligence.txt\n",
      "Length: 224\n",
      "--------------------\n",
      "Chunk from: Active_learning_(machine_learning).txt\n",
      "Length: 1000\n",
      "--------------------\n",
      "Chunk from: Active_learning_(machine_learning).txt\n",
      "Length: 894\n",
      "--------------------\n",
      "Chunk from: Active_learning_(machine_learning).txt\n",
      "Length: 94\n",
      "--------------------\n",
      "Chunk from: Adversarial_machine_learning.txt\n",
      "Length: 842\n",
      "--------------------\n",
      "Chunk from: Adversarial_machine_learning.txt\n",
      "Length: 42\n",
      "--------------------\n",
      "Chunk from: Affective_computing.txt\n",
      "Length: 828\n",
      "--------------------\n",
      "Chunk from: Affective_computing.txt\n",
      "Length: 28\n",
      "--------------------\n",
      "Chunk from: AI_boom.txt\n",
      "Length: 487\n",
      "--------------------\n",
      "Chunk from: AlexNet.txt\n",
      "Length: 1000\n",
      "--------------------\n",
      "Chunk from: AlexNet.txt\n",
      "Length: 456\n",
      "--------------------\n",
      "Chunk from: Applications_of_artificial_intelligence.txt\n",
      "Length: 590\n",
      "--------------------\n",
      "Chunk from: Artificial_general_intelligence.txt\n",
      "Length: 1000\n",
      "--------------------\n",
      "Chunk from: Artificial_general_intelligence.txt\n",
      "Length: 1000\n",
      "--------------------\n",
      "Chunk from: Artificial_general_intelligence.txt\n",
      "Length: 843\n",
      "--------------------\n",
      "Chunk from: Artificial_general_intelligence.txt\n",
      "Length: 43\n",
      "--------------------\n",
      "Chunk from: Artificial_intelligence.txt\n",
      "Length: 1000\n",
      "--------------------\n",
      "Chunk from: Artificial_intelligence.txt\n",
      "Length: 1000\n",
      "--------------------\n",
      "Chunk from: Artificial_intelligence.txt\n",
      "Length: 1000\n",
      "--------------------\n",
      "Chunk from: Artificial_intelligence.txt\n",
      "Length: 461\n",
      "--------------------\n",
      "Chunk from: Artificial_intelligence_art.txt\n",
      "Length: 1000\n",
      "--------------------\n",
      "Chunk from: Artificial_intelligence_art.txt\n",
      "Length: 298\n",
      "--------------------\n",
      "Chunk from: Artificial_intelligence_in_video_games.txt\n",
      "Length: 1000\n",
      "--------------------\n",
      "Chunk from: Artificial_intelligence_in_video_games.txt\n",
      "Length: 584\n",
      "--------------------\n",
      "Chunk from: Attention_(machine_learning).txt\n",
      "Length: 1000\n",
      "--------------------\n",
      "Chunk from: Attention_(machine_learning).txt\n",
      "Length: 594\n",
      "--------------------\n",
      "Chunk from: BERT_(language_model).txt\n",
      "Length: 1000\n",
      "--------------------\n",
      "Chunk from: BERT_(language_model).txt\n",
      "Length: 539\n",
      "--------------------\n",
      "Chunk from: ChatGPT.txt\n",
      "Length: 1000\n",
      "--------------------\n",
      "Chunk from: ChatGPT.txt\n",
      "Length: 1000\n",
      "--------------------\n",
      "Chunk from: ChatGPT.txt\n",
      "Length: 364\n",
      "--------------------\n",
      "Chunk from: Chinchilla_(language_model).txt\n",
      "Length: 177\n",
      "--------------------\n",
      "Chunk from: Chroma_(vector_database).txt\n",
      "Length: 396\n",
      "--------------------\n",
      "Chunk from: Claude_(language_model).txt\n",
      "Length: 547\n",
      "--------------------\n",
      "Chunk from: Computer_stereo_vision.txt\n",
      "Length: 378\n",
      "--------------------\n",
      "Chunk from: Computer_vision.txt\n",
      "Length: 1000\n",
      "--------------------\n",
      "Chunk from: Computer_vision.txt\n",
      "Length: 629\n",
      "--------------------\n",
      "Chunk from: Computer_Vision_Annotation_Tool.txt\n",
      "Length: 1000\n",
      "--------------------\n",
      "Chunk from: Computer_Vision_Annotation_Tool.txt\n",
      "Length: 291\n",
      "--------------------\n",
      "Chunk from: Computer_vision_dazzle.txt\n",
      "Length: 276\n",
      "--------------------\n",
      "Chunk from: Computer_vision_syndrome.txt\n",
      "Length: 325\n",
      "--------------------\n",
      "Chunk from: Convolutional_neural_network.txt\n",
      "Length: 1000\n",
      "--------------------\n",
      "Chunk from: Convolutional_neural_network.txt\n",
      "Length: 1000\n",
      "--------------------\n",
      "Chunk from: Convolutional_neural_network.txt\n",
      "Length: 1000\n",
      "--------------------\n",
      "Chunk from: Convolutional_neural_network.txt\n",
      "Length: 794\n",
      "--------------------\n",
      "Chunk from: Deeper_learning.txt\n",
      "Length: 849\n",
      "--------------------\n",
      "Chunk from: Deeper_learning.txt\n",
      "Length: 49\n",
      "--------------------\n",
      "Chunk from: Deep_learning.txt\n",
      "Length: 1000\n",
      "--------------------\n",
      "Chunk from: Deep_learning.txt\n",
      "Length: 682\n",
      "--------------------\n",
      "Chunk from: Deep_Learning_(South_Park).txt\n",
      "Length: 664\n",
      "--------------------\n",
      "Chunk from: Deep_Learning_Super_Sampling.txt\n",
      "Length: 888\n",
      "--------------------\n",
      "Chunk from: Deep_Learning_Super_Sampling.txt\n",
      "Length: 88\n",
      "--------------------\n",
      "Chunk from: Deep_reinforcement_learning.txt\n",
      "Length: 874\n",
      "--------------------\n",
      "Chunk from: Deep_reinforcement_learning.txt\n",
      "Length: 74\n",
      "--------------------\n",
      "Chunk from: Digital_image_processing.txt\n",
      "Length: 969\n",
      "--------------------\n",
      "Chunk from: Digital_image_processing.txt\n",
      "Length: 169\n",
      "--------------------\n",
      "Chunk from: Empirical_Methods_in_Natural_Language_Processing.txt\n",
      "Length: 832\n",
      "--------------------\n",
      "Chunk from: Empirical_Methods_in_Natural_Language_Processing.txt\n",
      "Length: 32\n",
      "--------------------\n",
      "Chunk from: Face_ID.txt\n",
      "Length: 1000\n",
      "--------------------\n",
      "Chunk from: Face_ID.txt\n",
      "Length: 1000\n",
      "--------------------\n",
      "Chunk from: Face_ID.txt\n",
      "Length: 1000\n",
      "--------------------\n",
      "Chunk from: Face_ID.txt\n",
      "Length: 610\n",
      "--------------------\n",
      "Chunk from: Face_perception.txt\n",
      "Length: 1000\n",
      "--------------------\n",
      "Chunk from: Face_perception.txt\n",
      "Length: 334\n",
      "--------------------\n",
      "Chunk from: Face_Recognition_Grand_Challenge.txt\n",
      "Length: 487\n",
      "--------------------\n",
      "Chunk from: Facial_recognition_system.txt\n",
      "Length: 1000\n",
      "--------------------\n",
      "Chunk from: Facial_recognition_system.txt\n",
      "Length: 1000\n",
      "--------------------\n",
      "Chunk from: Facial_recognition_system.txt\n",
      "Length: 676\n",
      "--------------------\n",
      "Chunk from: Feature_(computer_vision).txt\n",
      "Length: 1000\n",
      "--------------------\n",
      "Chunk from: Feature_(computer_vision).txt\n",
      "Length: 238\n",
      "--------------------\n",
      "Chunk from: Feedforward_neural_network.txt\n",
      "Length: 1000\n",
      "--------------------\n",
      "Chunk from: Feedforward_neural_network.txt\n",
      "Length: 200\n",
      "--------------------\n",
      "Chunk from: Fusiform_face_area.txt\n",
      "Length: 325\n",
      "--------------------\n",
      "Chunk from: Gemini_(language_model).txt\n",
      "Length: 432\n",
      "--------------------\n",
      "Chunk from: Generative_AI_pornography.txt\n",
      "Length: 507\n",
      "--------------------\n",
      "Chunk from: Generative_artificial_intelligence.txt\n",
      "Length: 1000\n",
      "--------------------\n",
      "Chunk from: Generative_artificial_intelligence.txt\n",
      "Length: 754\n",
      "--------------------\n",
      "Chunk from: Generative_pre-trained_transformer.txt\n",
      "Length: 1000\n",
      "--------------------\n",
      "Chunk from: Generative_pre-trained_transformer.txt\n",
      "Length: 703\n",
      "--------------------\n",
      "Chunk from: Gradient-domain_image_processing.txt\n",
      "Length: 474\n",
      "--------------------\n",
      "Chunk from: Graph_neural_network.txt\n",
      "Length: 1000\n",
      "--------------------\n",
      "Chunk from: Graph_neural_network.txt\n",
      "Length: 1000\n",
      "--------------------\n",
      "Chunk from: Graph_neural_network.txt\n",
      "Length: 647\n",
      "--------------------\n",
      "Chunk from: Hallucination_(artificial_intelligence).txt\n",
      "Length: 1000\n",
      "--------------------\n",
      "Chunk from: Hallucination_(artificial_intelligence).txt\n",
      "Length: 349\n",
      "--------------------\n",
      "Chunk from: History_of_artificial_intelligence.txt\n",
      "Length: 1000\n",
      "--------------------\n",
      "Chunk from: History_of_artificial_intelligence.txt\n",
      "Length: 1000\n",
      "--------------------\n",
      "Chunk from: History_of_artificial_intelligence.txt\n",
      "Length: 1000\n",
      "--------------------\n",
      "Chunk from: History_of_artificial_intelligence.txt\n",
      "Length: 283\n",
      "--------------------\n",
      "Chunk from: History_of_artificial_neural_networks.txt\n",
      "Length: 1000\n",
      "--------------------\n",
      "Chunk from: History_of_artificial_neural_networks.txt\n",
      "Length: 500\n",
      "--------------------\n",
      "Chunk from: History_of_natural_language_processing.txt\n",
      "Length: 297\n",
      "--------------------\n",
      "Chunk from: Homography_(computer_vision).txt\n",
      "Length: 656\n",
      "--------------------\n",
      "Chunk from: Image.txt\n",
      "Length: 1000\n",
      "--------------------\n",
      "Chunk from: Image.txt\n",
      "Length: 646\n",
      "--------------------\n",
      "Chunk from: Image_processor.txt\n",
      "Length: 610\n",
      "--------------------\n",
      "Chunk from: Kernel_(image_processing).txt\n",
      "Length: 421\n",
      "--------------------\n",
      "Chunk from: LangChain.txt\n",
      "Length: 348\n",
      "--------------------\n",
      "Chunk from: Language_model.txt\n",
      "Length: 720\n",
      "--------------------\n",
      "Chunk from: Large_language_model.txt\n",
      "Length: 667\n",
      "--------------------\n",
      "Chunk from: Large_language_models_in_government.txt\n",
      "Length: 147\n",
      "--------------------\n",
      "Chunk from: List_of_datasets_for_machine-learning_research.txt\n",
      "Length: 1000\n",
      "--------------------\n",
      "Chunk from: List_of_datasets_for_machine-learning_research.txt\n",
      "Length: 457\n",
      "--------------------\n",
      "Chunk from: List_of_datasets_in_computer_vision_and_image_processing.txt\n",
      "Length: 338\n",
      "--------------------\n",
      "Chunk from: List_of_large_language_models.txt\n",
      "Length: 353\n",
      "--------------------\n",
      "Chunk from: Llama_(language_model).txt\n",
      "Length: 985\n",
      "--------------------\n",
      "Chunk from: Llama_(language_model).txt\n",
      "Length: 185\n",
      "--------------------\n",
      "Chunk from: Machine_learning.txt\n",
      "Length: 1000\n",
      "--------------------\n",
      "Chunk from: Machine_learning.txt\n",
      "Length: 289\n",
      "--------------------\n",
      "Chunk from: Microscope_image_processing.txt\n",
      "Length: 514\n",
      "--------------------\n",
      "Chunk from: Moving_object_detection.txt\n",
      "Length: 456\n",
      "--------------------\n",
      "Chunk from: Music_and_artificial_intelligence.txt\n",
      "Length: 1000\n",
      "--------------------\n",
      "Chunk from: Music_and_artificial_intelligence.txt\n",
      "Length: 995\n",
      "--------------------\n",
      "Chunk from: Music_and_artificial_intelligence.txt\n",
      "Length: 195\n",
      "--------------------\n",
      "Chunk from: Natural-language_user_interface.txt\n",
      "Length: 949\n",
      "--------------------\n",
      "Chunk from: Natural-language_user_interface.txt\n",
      "Length: 149\n",
      "--------------------\n",
      "Chunk from: Natural_language.txt\n",
      "Length: 455\n",
      "--------------------\n",
      "Chunk from: Natural_language_processing.txt\n",
      "Length: 556\n",
      "--------------------\n",
      "Chunk from: Natural_Language_Toolkit.txt\n",
      "Length: 1000\n",
      "--------------------\n",
      "Chunk from: Natural_Language_Toolkit.txt\n",
      "Length: 336\n",
      "--------------------\n",
      "Chunk from: NebulaGraph.txt\n",
      "Length: 257\n",
      "--------------------\n",
      "Chunk from: Neural_network.txt\n",
      "Length: 700\n",
      "--------------------\n",
      "Chunk from: Neural_network_(biology).txt\n",
      "Length: 549\n",
      "--------------------\n",
      "Chunk from: Neural_network_(machine_learning).txt\n",
      "Length: 1000\n",
      "--------------------\n",
      "Chunk from: Neural_network_(machine_learning).txt\n",
      "Length: 854\n",
      "--------------------\n",
      "Chunk from: Neural_network_(machine_learning).txt\n",
      "Length: 54\n",
      "--------------------\n",
      "Chunk from: Normalization_(image_processing).txt\n",
      "Length: 1000\n",
      "--------------------\n",
      "Chunk from: Normalization_(image_processing).txt\n",
      "Length: 1000\n",
      "--------------------\n",
      "Chunk from: Normalization_(image_processing).txt\n",
      "Length: 1000\n",
      "--------------------\n",
      "Chunk from: Normalization_(image_processing).txt\n",
      "Length: 1000\n",
      "--------------------\n",
      "Chunk from: Normalization_(image_processing).txt\n",
      "Length: 1000\n",
      "--------------------\n",
      "Chunk from: Normalization_(image_processing).txt\n",
      "Length: 1000\n",
      "--------------------\n",
      "Chunk from: Normalization_(image_processing).txt\n",
      "Length: 300\n",
      "--------------------\n",
      "Chunk from: Object_detection.txt\n",
      "Length: 476\n",
      "--------------------\n",
      "Chunk from: OpenAI_o1.txt\n",
      "Length: 351\n",
      "--------------------\n",
      "Chunk from: Outline_of_natural_language_processing.txt\n",
      "Length: 820\n",
      "--------------------\n",
      "Chunk from: Outline_of_natural_language_processing.txt\n",
      "Length: 20\n",
      "--------------------\n",
      "Chunk from: Outline_of_object_recognition.txt\n",
      "Length: 630\n",
      "--------------------\n",
      "Chunk from: Pareidolia.txt\n",
      "Length: 889\n",
      "--------------------\n",
      "Chunk from: Pareidolia.txt\n",
      "Length: 89\n",
      "--------------------\n",
      "Chunk from: Physics-informed_neural_networks.txt\n",
      "Length: 1000\n",
      "--------------------\n",
      "Chunk from: Physics-informed_neural_networks.txt\n",
      "Length: 217\n",
      "--------------------\n",
      "Chunk from: Pose_(computer_vision).txt\n",
      "Length: 757\n",
      "--------------------\n",
      "Chunk from: Prompt_engineering.txt\n",
      "Length: 1000\n",
      "--------------------\n",
      "Chunk from: Prompt_engineering.txt\n",
      "Length: 220\n",
      "--------------------\n",
      "Chunk from: Prosopagnosia.txt\n",
      "Length: 561\n",
      "--------------------\n",
      "Chunk from: Pyramid_(image_processing).txt\n",
      "Length: 392\n",
      "--------------------\n",
      "Chunk from: Q-learning.txt\n",
      "Length: 1000\n",
      "--------------------\n",
      "Chunk from: Q-learning.txt\n",
      "Length: 282\n",
      "--------------------\n",
      "Chunk from: Quantum_machine_learning.txt\n",
      "Length: 1000\n",
      "--------------------\n",
      "Chunk from: Quantum_machine_learning.txt\n",
      "Length: 943\n",
      "--------------------\n",
      "Chunk from: Quantum_machine_learning.txt\n",
      "Length: 143\n",
      "--------------------\n",
      "Chunk from: Quantum_natural_language_processing.txt\n",
      "Length: 466\n",
      "--------------------\n",
      "Chunk from: Reasoning_language_model.txt\n",
      "Length: 343\n",
      "--------------------\n",
      "Chunk from: Rectifier_(neural_networks).txt\n",
      "Length: 1000\n",
      "--------------------\n",
      "Chunk from: Rectifier_(neural_networks).txt\n",
      "Length: 973\n",
      "--------------------\n",
      "Chunk from: Rectifier_(neural_networks).txt\n",
      "Length: 173\n",
      "--------------------\n",
      "Chunk from: Recurrent_neural_network.txt\n",
      "Length: 1000\n",
      "--------------------\n",
      "Chunk from: Recurrent_neural_network.txt\n",
      "Length: 1000\n",
      "--------------------\n",
      "Chunk from: Recurrent_neural_network.txt\n",
      "Length: 315\n",
      "--------------------\n",
      "Chunk from: Retrieval-augmented_generation.txt\n",
      "Length: 1000\n",
      "--------------------\n",
      "Chunk from: Retrieval-augmented_generation.txt\n",
      "Length: 1000\n",
      "--------------------\n",
      "Chunk from: Retrieval-augmented_generation.txt\n",
      "Length: 691\n",
      "--------------------\n",
      "Chunk from: Runway_(company).txt\n",
      "Length: 744\n",
      "--------------------\n",
      "Chunk from: Semantic_decomposition_(natural_language_processing).txt\n",
      "Length: 753\n",
      "--------------------\n",
      "Chunk from: Small_object_detection.txt\n",
      "Length: 394\n",
      "--------------------\n",
      "Chunk from: Supervised_learning.txt\n",
      "Length: 691\n",
      "--------------------\n",
      "Chunk from: T5_(language_model).txt\n",
      "Length: 702\n",
      "--------------------\n",
      "Chunk from: Three-dimensional_face_recognition.txt\n",
      "Length: 1000\n",
      "--------------------\n",
      "Chunk from: Three-dimensional_face_recognition.txt\n",
      "Length: 1000\n",
      "--------------------\n",
      "Chunk from: Three-dimensional_face_recognition.txt\n",
      "Length: 284\n",
      "--------------------\n",
      "Chunk from: Thresholding_(image_processing).txt\n",
      "Length: 209\n",
      "--------------------\n",
      "Chunk from: Timeline_of_artificial_intelligence.txt\n",
      "Length: 154\n",
      "--------------------\n",
      "Chunk from: Transformer_(deep_learning_architecture).txt\n",
      "Length: 1000\n",
      "--------------------\n",
      "Chunk from: Transformer_(deep_learning_architecture).txt\n",
      "Length: 681\n",
      "--------------------\n",
      "Chunk from: Triangulation_(computer_vision).txt\n",
      "Length: 1000\n",
      "--------------------\n",
      "Chunk from: Triangulation_(computer_vision).txt\n",
      "Length: 937\n",
      "--------------------\n",
      "Chunk from: Triangulation_(computer_vision).txt\n",
      "Length: 137\n",
      "--------------------\n",
      "Chunk from: Vector_database.txt\n",
      "Length: 1000\n",
      "--------------------\n",
      "Chunk from: Vector_database.txt\n",
      "Length: 1000\n",
      "--------------------\n",
      "Chunk from: Vector_database.txt\n",
      "Length: 502\n",
      "--------------------\n",
      "Chunk from: Video_content_analysis.txt\n",
      "Length: 1000\n",
      "--------------------\n",
      "Chunk from: Video_content_analysis.txt\n",
      "Length: 470\n",
      "--------------------\n",
      "Chunk from: Viola–Jones_object_detection_framework.txt\n",
      "Length: 1000\n",
      "--------------------\n",
      "Chunk from: Viola–Jones_object_detection_framework.txt\n",
      "Length: 676\n",
      "--------------------\n",
      "Chunk from: Watershed_(image_processing).txt\n",
      "Length: 1000\n",
      "--------------------\n",
      "Chunk from: Watershed_(image_processing).txt\n",
      "Length: 247\n",
      "--------------------\n",
      "Chunk from: You_Only_Look_Once.txt\n",
      "Length: 583\n",
      "--------------------\n",
      "\n",
      "Chunking process complete. Ready for embedding generation.\n"
     ]
    }
   ],
   "source": [
    "dataset_folder = \"dataset\"\n",
    "chunk_size = 1000  # You can adjust this\n",
    "chunk_overlap = 200  # You can adjust this\n",
    "\n",
    "# Iterate through all files in the dataset folder\n",
    "for filename in os.listdir(dataset_folder):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        filepath = os.path.join(dataset_folder, filename)\n",
    "        try:\n",
    "            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                content = f.read()\n",
    "                # Split the content into chunks and process each chunk\n",
    "                for i in range(0, len(content), chunk_size - chunk_overlap):\n",
    "                    chunk = content[i:i + chunk_size]\n",
    "                    metadata = {\"source\": filename}\n",
    "                    # For now, let's just print the chunk and its metadata\n",
    "                    print(f\"Chunk from: {metadata['source']}\")\n",
    "                    print(f\"Length: {len(chunk)}\")\n",
    "                    # print(f\"Content: {chunk[:50]}...\") # Print first 50 characters of the chunk\n",
    "                    print(\"-\" * 20)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file '{filename}': {e}\")\n",
    "\n",
    "print(\"\\nChunking process complete. Ready for embedding generation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cecfe1a-d70c-49ea-9292-b823e1a0c8df",
   "metadata": {},
   "source": [
    "#### Generate embeddings\n",
    "\n",
    "* Load a Sentence Transformer Model: We'll choose a suitable pre-trained model from the sentence-transformers library. These models are specifically designed for generating sentence and text embeddings.\n",
    "* Generate Embeddings for Each Chunk: We'll iterate through our documents, chunk them as before, and then use the loaded model to generate an embedding vector for each chunk.\n",
    "* Store Embeddings in a Vector Database: We'll use FAISS (which we installed earlier) to create an index and store the generated embedding vectors along with their corresponding metadata (e.g., the source filename and the original chunk content). FAISS allows for efficient similarity searching later when we have a user's question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "290d82ad-e10e-43d1-804a-57859b63cd41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Transformer model loaded successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the pre-trained Sentence Transformer model\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"Sentence Transformer model loaded successfully.\")\n",
    "embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "100cb5b4-b5b6-4f79-a736-e2c27a4a32d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dimension = embedding_model.get_sentence_embedding_dimension()\n",
    "embedding_dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "86291386-a487-4fca-a49d-bfbdc097968d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<faiss.swigfaiss_avx2.IndexFlatL2; proxy of <Swig Object of type 'faiss::IndexFlatL2 *' at 0x00000241DC666460> >"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize FAISS index\n",
    "index = faiss.IndexFlatL2(embedding_dimension)  # Using L2 distance for similarity\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fca4d16b-c800-4da8-abd9-da7e479ccc4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of chunks: 190\n",
      "FAISS index size: 190\n",
      "Embeddings generated and added to FAISS index.\n",
      "FAISS index saved to 'document_embeddings.faiss'\n",
      "Chunks and metadata saved to 'chunks_metadata.json'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lists to store chunks and their metadata (for now, just source)\n",
    "chunks = []\n",
    "metadata = []\n",
    "\n",
    "# Iterate through all files in the dataset folder\n",
    "for filename in os.listdir(dataset_folder):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        filepath = os.path.join(dataset_folder, filename)\n",
    "        try:\n",
    "            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                content = f.read()\n",
    "                # Split the content into chunks\n",
    "                for i in range(0, len(content), chunk_size - chunk_overlap):\n",
    "                    chunk = content[i:i + chunk_size]\n",
    "                    chunks.append(chunk)\n",
    "                    metadata.append({\"source\": filename})\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file '{filename}': {e}\")\n",
    "\n",
    "# Generate embeddings for all chunks\n",
    "embeddings = embedding_model.encode(chunks)\n",
    "# Add embeddings to the FAISS index\n",
    "index.add(np.array(embeddings).astype('float32'))\n",
    "\n",
    "print(f\"Total number of chunks: {len(chunks)}\")\n",
    "print(f\"FAISS index size: {index.ntotal}\")\n",
    "print(\"Embeddings generated and added to FAISS index.\")\n",
    "\n",
    "# Now, 'index' contains the embeddings of our document chunks.\n",
    "# We can save this index for later use if needed.\n",
    "faiss.write_index(index, \"document_embeddings.faiss\")\n",
    "print(\"FAISS index saved to 'document_embeddings.faiss'\")\n",
    "\n",
    "# We also need to save the chunks and metadata, perhaps in a separate file\n",
    "with open(\"chunks_metadata.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"chunks\": chunks, \"metadata\": metadata}, f)\n",
    "print(\"Chunks and metadata saved to 'chunks_metadata.json'\")\n",
    "\n",
    "# Explicitly delete the large variables\n",
    "del chunks\n",
    "del embeddings\n",
    "del metadata\n",
    "\n",
    "# Trigger garbage collection\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bc8f3b-2a0f-4f0b-abd2-38240912427f",
   "metadata": {},
   "source": [
    "Explanation of the above code:\n",
    "\n",
    "* embedding_dimension = embedding_model.get_sentence_embedding_dimension(): We get the dimensionality of the embeddings produced by our chosen Sentence Transformer model. This is needed to initialize the FAISS index.\n",
    "* index = faiss.IndexFlatL2(embedding_dimension): We create a FAISS index. IndexFlatL2 is a simple index that performs an exact (non-approximate) nearest neighbor search using the L2 distance (Euclidean distance). For larger datasets, we might consider more advanced indexing techniques for faster search, but for our learning project, this is a good starting point. We still load and chunk the documents as before, storing the chunks and their metadata in lists.\n",
    "* embeddings = embedding_model.encode(chunks): We use the loaded Sentence Transformer model to generate embeddings for all the chunks in one go. This returns a NumPy array of embeddings.\n",
    "* index.add(np.array(embeddings).astype('float32')): We add the generated embeddings (converted to a NumPy array of float32) to our FAISS index. We print the total number of chunks and the size of the FAISS index to verify the process.\n",
    "* faiss.write_index(index, \"document_embeddings.faiss\"): We save the FAISS index to a file so we can load it later without re-generating the embeddings. We also save the chunks and metadata to a JSON file. While the FAISS index stores the embeddings, we'll need the actual text chunks and their source information later when we retrieve context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "659bb5c4-bde7-4405-a9fc-fe75867eee01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is trained: True\n",
      "Number of vectors: 190\n",
      "Dimension of vectors: 384\n",
      "Index type: <class 'faiss.swigfaiss_avx2.IndexFlatL2'>\n"
     ]
    }
   ],
   "source": [
    "# Printing the FAISS index information\n",
    "print(f\"Is trained: {index.is_trained}\")\n",
    "print(f\"Number of vectors: {index.ntotal}\")\n",
    "print(f\"Dimension of vectors: {index.d}\")\n",
    "print(f\"Index type: {type(index)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4aabf42-c762-4976-8c4e-e681f1cb3ed9",
   "metadata": {},
   "source": [
    "Based on above output we have a FAISS index containing 2969 semantic representations of your document chunks, each represented by a 384-dimensional vector, and the search will be based on exact L2 distance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1257f97a-7cf2-4a26-b331-d4a9762c50fc",
   "metadata": {},
   "source": [
    "#### Implementing the retrieval mechanism.\n",
    "A function that takes a user's question as input, embeds it using our embedding_model, searches the FAISS index for the most similar embeddings, and returns the corresponding text chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f572b15f-08ba-490e-b390-5bb065985344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the chunks and metadata\n",
    "with open(\"chunks_metadata.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    chunks_data = json.load(f)\n",
    "    chunks = chunks_data['chunks']\n",
    "    metadata = chunks_data['metadata']\n",
    "\n",
    "def retrieve_relevant_chunks(query, top_k=3):\n",
    "    \"\"\"\n",
    "    Embeds the query and retrieves the top_k most relevant document chunks from the FAISS index.\n",
    "\n",
    "    Args:\n",
    "        query (str): The user's question.\n",
    "        top_k (int): The number of top relevant chunks to retrieve.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of the top_k most relevant text chunks and their metadata.\n",
    "    \"\"\"\n",
    "    # 1. Embed the query\n",
    "    query_embedding = embedding_model.encode([query]).astype('float32')\n",
    "\n",
    "    # 2. Search the FAISS index\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "\n",
    "    # 3. Retrieve the corresponding text chunks and metadata\n",
    "    relevant_chunks = [chunks[i] for i in indices[0]]\n",
    "    relevant_metadata = [metadata[i] for i in indices[0]]\n",
    "\n",
    "    return relevant_chunks, relevant_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "084fbd0f-7c3f-40a9-a13b-b55abd35f76f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Explain the architecture of a large language model.\n",
      "\n",
      "Top 3 relevant chunks:\n",
      "\n",
      "Chunk 1:\n",
      "Title: Large language models in government\n",
      "\n",
      "Summary:\n",
      "Large language models have been used by officials and politicians in a wide variety of ways.\n",
      "\n",
      "...\n",
      "Source: Large_language_models_in_government.txt\n",
      "\n",
      "----------------------------------------\n",
      "Chunk 2:\n",
      "Title: Language model\n",
      "\n",
      "Summary:\n",
      "A language model is a model of natural language. Language models are useful for a variety of tasks, including speech recognition, machine translation, natural language ...\n",
      "Source: Language_model.txt\n",
      "\n",
      "----------------------------------------\n",
      "Chunk 3:\n",
      "Title: List of large language models\n",
      "\n",
      "Summary:\n",
      "A large language model (LLM) is a type of machine learning model designed for natural language processing tasks such as language generation. LLMs are lan...\n",
      "Source: List_of_large_language_models.txt\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Query: What are some applications of generative AI?\n",
      "\n",
      "Top 3 relevant chunks:\n",
      "\n",
      "Chunk 1:\n",
      "Title: Generative artificial intelligence\n",
      "\n",
      "Summary:\n",
      "Generative artificial intelligence (Generative AI, GenAI, or GAI) is a subfield of artificial intelligence that uses generative models to produce te...\n",
      "Source: Generative_artificial_intelligence.txt\n",
      "\n",
      "----------------------------------------\n",
      "Chunk 2:\n",
      "ffusion, Midjourney, and DALL-E; and text-to-video AI generators such as Sora. Technology companies developing generative AI include OpenAI, Anthropic, Microsoft, Google, and Baidu.\n",
      "Generative AI has ...\n",
      "Source: Generative_artificial_intelligence.txt\n",
      "\n",
      "----------------------------------------\n",
      "Chunk 3:\n",
      "Title: Artificial intelligence\n",
      "\n",
      "Summary:\n",
      "Artificial intelligence (AI) refers to the capability of computational systems to perform tasks typically associated with human intelligence, such as learning,...\n",
      "Source: Artificial_intelligence.txt\n",
      "\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Example usage in Jupyter Notebook:\n",
    "query_1 = \"Explain the architecture of a large language model.\"\n",
    "relevant_chunks, relevant_metadata = retrieve_relevant_chunks(query_1)\n",
    "\n",
    "print(f\"Query: {query_1}\\n\")\n",
    "print(f\"Top {len(relevant_chunks)} relevant chunks:\\n\")\n",
    "for i, chunk in enumerate(relevant_chunks):\n",
    "    print(f\"Chunk {i+1}:\")\n",
    "    print(chunk[:200] + \"...\") # Print the first 200 characters\n",
    "    print(f\"Source: {relevant_metadata[i]['source']}\\n\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# You can try different queries here\n",
    "query_2 = \"What are some applications of generative AI?\"\n",
    "relevant_chunks_2, relevant_metadata_2 = retrieve_relevant_chunks(query_2)\n",
    "\n",
    "print(f\"\\nQuery: {query_2}\\n\")\n",
    "print(f\"Top {len(relevant_chunks_2)} relevant chunks:\\n\")\n",
    "for i, chunk in enumerate(relevant_chunks_2):\n",
    "    print(f\"Chunk {i+1}:\")\n",
    "    print(chunk[:200] + \"...\")\n",
    "    print(f\"Source: {relevant_metadata_2[i]['source']}\\n\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968c1fc7-1fcd-4d35-9ec1-31ce7c1b00a7",
   "metadata": {},
   "source": [
    "The code above doest the following -\n",
    "* It loads the tools: It loads the Sentence Transformer model that we used to understand the meaning of our documents and the FAISS index where we stored the numerical representations (embeddings) of those documents. It also loads the original text chunks and some information about where they came from.\n",
    "* It understands your question: When you give it a query, it uses the same Sentence Transformer model to convert your question into a numerical representation (an embedding). This way, the computer can understand the meaning of your question in a numerical form.\n",
    "* It searches for similar information: It then uses this numerical representation of your question to search the FAISS index. The FAISS index is like a super-fast lookup system that finds the document embeddings that are most similar in meaning to your question's embedding. It returns the top few most similar ones (we set it to top_k=3 by default).\n",
    "* It finds the original text: For each of the most similar embeddings found in the FAISS index, the code looks up the original text chunk that corresponds to that embedding. It also retrieves any extra information we saved about that chunk (like which document it came from).\n",
    "* It gives you the results: Finally, the function returns a list of these most relevant text chunks and their associated information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a19639-0ec5-4048-b1cf-ebb5a30114f3",
   "metadata": {},
   "source": [
    "####  Integrate a Language Model (LLM) \n",
    "To generate an answer using the retrieved context and the original question.\n",
    "\n",
    "Steps:\n",
    "* Load a Pre-trained Language Model: We'll use a suitable pre-trained language model from the transformers library.\n",
    "* Construct a Prompt: We need to create a prompt that we will feed to the LLM. This prompt will typically include:\n",
    "    * The user's question.\n",
    "    * The retrieved relevant context (the text chunks from our knowledge base).\n",
    "    * Instructions for the LLM on how to use the context to answer the question.\n",
    "* Generate the Answer: We'll pass the constructed prompt to the LLM and ask it to generate a response.\n",
    "* Format the Output: We'll then present the generated answer to the user, potentially along with the sources (the documents from which the context was retrieved)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2c460fed-662f-4f80-8c70-6c1ddb209dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForQuestionAnswering were not initialized from the model checkpoint at gpt2-large and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 model loaded successfully for question answering.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<transformers.pipelines.question_answering.QuestionAnsweringPipeline at 0x241d9f04890>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the pre-trained GPT-2 model and tokenizer using the pipeline\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"gpt2-large\")\n",
    "print(\"GPT-2 model loaded successfully for question answering.\")\n",
    "qa_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7c5a8c94-095d-4f8d-9aa7-96a742f0e378",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(query, context_chunks):\n",
    "    \"\"\"\n",
    "    Uses the GPT-2 model to answer a question based on the provided context chunks.\n",
    "\n",
    "    Args:\n",
    "        query (str): The user's question.\n",
    "        context_chunks (list): A list of relevant text chunks.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated answer.\n",
    "    \"\"\"\n",
    "    if not context_chunks:\n",
    "        return \"No relevant context found to answer the question.\"\n",
    "\n",
    "    # Combine the context chunks into a single string\n",
    "    context = \" \".join(context_chunks)\n",
    "\n",
    "    # Construct the input for the question-answering pipeline\n",
    "    input_data = {\n",
    "        'question': query,\n",
    "        'context': context\n",
    "    }\n",
    "\n",
    "    #We can also give it a prompt in the form of string\n",
    "    #prompt = f\"Answer the following question based on the context provided.\\n\\nContext:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
    "\n",
    "    # Get the answer from the pipeline\n",
    "    result = qa_pipeline(input_data)\n",
    "    #result = qa_pipeline(prompt)\n",
    "    return result['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ba5830cf-f3e9-47ce-966a-22237804e494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Explain the architecture of a large language model and its key components.\n",
      "\n",
      "Retrieved Context Chunks:\n",
      "\n",
      "Chunk 1 (Source: Language_model.txt):\n",
      "Title: Language model\n",
      "\n",
      "Summary:\n",
      "A language model is a model of natural language. Language models are useful for a variety of tasks, including speech recognition, machine translation, natural language ...\n",
      "----------------------------------------\n",
      "Chunk 2 (Source: Large_language_models_in_government.txt):\n",
      "Title: Large language models in government\n",
      "\n",
      "Summary:\n",
      "Large language models have been used by officials and politicians in a wide variety of ways.\n",
      "\n",
      "...\n",
      "----------------------------------------\n",
      "Chunk 3 (Source: List_of_large_language_models.txt):\n",
      "Title: List of large language models\n",
      "\n",
      "Summary:\n",
      "A large language model (LLM) is a type of machine learning model designed for natural language processing tasks such as language generation. LLMs are lan...\n",
      "----------------------------------------\n",
      "\n",
      "Generated Answer:  character\n"
     ]
    }
   ],
   "source": [
    "query = \"Explain the architecture of a large language model and its key components.\"\n",
    "relevant_chunks, relevant_metadata = retrieve_relevant_chunks(query)\n",
    "\n",
    "print(f\"Question: {query}\\n\")\n",
    "print(\"Retrieved Context Chunks:\\n\")\n",
    "for i, chunk in enumerate(relevant_chunks):\n",
    "    print(f\"Chunk {i+1} (Source: {relevant_metadata[i]['source']}):\")\n",
    "    print(chunk[:200] + \"...\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "answer = answer_question(query, relevant_chunks)\n",
    "print(f\"\\nGenerated Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fb4775d0-9846-4798-866f-abcc00b8730a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Explain what is a large language model in machine learning\n",
      "\n",
      "Retrieved Context Chunks:\n",
      "\n",
      "Chunk 1 (Source: List_of_large_language_models.txt):\n",
      "Title: List of large language models\n",
      "\n",
      "Summary:\n",
      "A large language model (LLM) is a type of machine learning model designed for natural language processing tasks such as language generation. LLMs are lan...\n",
      "----------------------------------------\n",
      "Chunk 2 (Source: Language_model.txt):\n",
      "Title: Language model\n",
      "\n",
      "Summary:\n",
      "A language model is a model of natural language. Language models are useful for a variety of tasks, including speech recognition, machine translation, natural language ...\n",
      "----------------------------------------\n",
      "Chunk 3 (Source: Large_language_model.txt):\n",
      "Title: Large language model\n",
      "\n",
      "Summary:\n",
      "A large language model (LLM) is a type of machine learning model designed for natural language processing tasks such as language generation. LLMs are language mod...\n",
      "----------------------------------------\n",
      "\n",
      "Generated Answer:  character\n"
     ]
    }
   ],
   "source": [
    "query = \"Explain what is a large language model in machine learning\"\n",
    "relevant_chunks, relevant_metadata = retrieve_relevant_chunks(query)\n",
    "\n",
    "print(f\"Question: {query}\\n\")\n",
    "print(\"Retrieved Context Chunks:\\n\")\n",
    "for i, chunk in enumerate(relevant_chunks):\n",
    "    print(f\"Chunk {i+1} (Source: {relevant_metadata[i]['source']}):\")\n",
    "    print(chunk[:200] + \"...\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "answer = answer_question(query, relevant_chunks)\n",
    "print(f\"\\nGenerated Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42ea664-5f0a-4c4c-8921-05c62a0b22fe",
   "metadata": {},
   "source": [
    "#### Tuning to get desired results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "86eca357-5873-4fe8-bce3-308222ebc9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in d:\\extras\\anaconda\\envs\\contextcraft_qa\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in d:\\extras\\anaconda\\envs\\contextcraft_qa\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in d:\\extras\\anaconda\\envs\\contextcraft_qa\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\extras\\anaconda\\envs\\contextcraft_qa\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in d:\\extras\\anaconda\\envs\\contextcraft_qa\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in d:\\extras\\anaconda\\envs\\contextcraft_qa\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: huggingface_hub[hf_xet] in d:\\extras\\anaconda\\envs\\contextcraft_qa\\lib\\site-packages (0.30.2)\n",
      "Requirement already satisfied: filelock in d:\\extras\\anaconda\\envs\\contextcraft_qa\\lib\\site-packages (from huggingface_hub[hf_xet]) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\extras\\anaconda\\envs\\contextcraft_qa\\lib\\site-packages (from huggingface_hub[hf_xet]) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in d:\\extras\\anaconda\\envs\\contextcraft_qa\\lib\\site-packages (from huggingface_hub[hf_xet]) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\extras\\anaconda\\envs\\contextcraft_qa\\lib\\site-packages (from huggingface_hub[hf_xet]) (6.0.2)\n",
      "Requirement already satisfied: requests in d:\\extras\\anaconda\\envs\\contextcraft_qa\\lib\\site-packages (from huggingface_hub[hf_xet]) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in d:\\extras\\anaconda\\envs\\contextcraft_qa\\lib\\site-packages (from huggingface_hub[hf_xet]) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\extras\\anaconda\\envs\\contextcraft_qa\\lib\\site-packages (from huggingface_hub[hf_xet]) (4.12.2)\n",
      "Requirement already satisfied: hf-xet>=0.1.4 in d:\\extras\\anaconda\\envs\\contextcraft_qa\\lib\\site-packages (from huggingface_hub[hf_xet]) (1.0.5)\n",
      "Requirement already satisfied: colorama in d:\\extras\\anaconda\\envs\\contextcraft_qa\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub[hf_xet]) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\extras\\anaconda\\envs\\contextcraft_qa\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\extras\\anaconda\\envs\\contextcraft_qa\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\extras\\anaconda\\envs\\contextcraft_qa\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\extras\\anaconda\\envs\\contextcraft_qa\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install huggingface_hub[hf_xet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "683e74d2-d9cc-423f-8574-5b3822eff6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Download punkt tokenizer if not already downloaded\n",
    "try:\n",
    "    sent_tokenize(\"This is a test sentence.\")\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "016d7aee-cad4-486d-b521-91ac626189fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = \"dataset\"\n",
    "chunk_size = 500  # Adjust chunk size for sentences\n",
    "chunk_overlap = 50  # Adjust overlap for sentences\n",
    "\n",
    "all_chunks = []\n",
    "all_metadata = []\n",
    "\n",
    "# Iterate through all files in the dataset folder\n",
    "for filename in os.listdir(dataset_folder):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        filepath = os.path.join(dataset_folder, filename)\n",
    "        try:\n",
    "            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                content = f.read()\n",
    "                sentences = sent_tokenize(content)\n",
    "                current_chunk = \"\"\n",
    "                for sentence in sentences:\n",
    "                    if len(current_chunk) + len(sentence) + 1 <= chunk_size:\n",
    "                        current_chunk += sentence + \" \"\n",
    "                    else:\n",
    "                        all_chunks.append(current_chunk.strip())\n",
    "                        all_metadata.append({\"source\": filename})\n",
    "                        current_chunk = sentence + \" \"\n",
    "                if current_chunk:\n",
    "                    all_chunks.append(current_chunk.strip())\n",
    "                    all_metadata.append({\"source\": filename})\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file '{filename}': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "d613df31-4256-4761-ad67-78cfe6cbbeb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Extras\\Anaconda\\envs\\contextcraft_qa\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Abhi K Thakkar\\.cache\\huggingface\\hub\\models--sentence-transformers--multi-qa-mpnet-base-dot-v1. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 284 chunks using sentence-based splitting.\n",
      "Embeddings saved to document_embeddings_sentences_mpnet.faiss using all-mpnet-base-v2\n",
      "Metadata saved to chunks_metadata_sentences_mpnet.json\n"
     ]
    }
   ],
   "source": [
    "# Load a different Sentence Transformer model\n",
    "embedding_model = SentenceTransformer('multi-qa-mpnet-base-dot-v1')\n",
    "embeddings = embedding_model.encode(all_chunks)\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embeddings)\n",
    "faiss.write_index(index, \"document_embeddings_sentences_mpnet.faiss\")\n",
    "\n",
    "chunks_metadata = {\"chunks\": all_chunks, \"metadata\": all_metadata}\n",
    "with open(\"chunks_metadata_sentences_mpnet.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(chunks_metadata, f)\n",
    "\n",
    "print(f\"Processed {len(all_chunks)} chunks using sentence-based splitting.\")\n",
    "print(f\"Embeddings saved to document_embeddings_sentences_mpnet.faiss using all-mpnet-base-v2\")\n",
    "print(f\"Metadata saved to chunks_metadata_sentences_mpnet.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "2f05dae4-a68c-48f1-ba2a-84046433e964",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 model loaded successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<transformers.pipelines.text_generation.TextGenerationPipeline at 0x241deb43c50>"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = faiss.read_index(\"document_embeddings_sentences_mpnet.faiss\")\n",
    "\n",
    "# Load the chunks and metadata\n",
    "with open(\"chunks_metadata_sentences_mpnet.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    chunks_data = json.load(f)\n",
    "    chunks = chunks_data['chunks']\n",
    "    metadata = chunks_data['metadata']\n",
    "\n",
    "# Load the pre-trained GPT-2 model and tokenizer using the pipeline\n",
    "text_generator = pipeline(\"text-generation\", model=\"gpt2-large\") #For test generation instead of question answering.\n",
    "print(\"GPT-2 model loaded successfully.\")\n",
    "text_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "47d9f2d2-ab07-4df2-b159-a83e270a8b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_chunks(query, top_k=2):\n",
    "    \"\"\"\n",
    "    Embeds the query and retrieves the top_k most relevant document chunks from the FAISS index.\n",
    "\n",
    "    Args:\n",
    "        query (str): The user's question.\n",
    "        top_k (int): The number of top relevant chunks to retrieve.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of the top_k most relevant text chunks and their metadata.\n",
    "    \"\"\"\n",
    "    # 1. Embed the query\n",
    "    query_embedding = embedding_model.encode([query]).astype('float32')\n",
    "\n",
    "    # 2. Search the FAISS index\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "\n",
    "    # 3. Retrieve the corresponding text chunks and metadata\n",
    "    relevant_chunks = [chunks[i] for i in indices[0]]\n",
    "    relevant_metadata = [metadata[i] for i in indices[0]]\n",
    "\n",
    "    return relevant_chunks, relevant_metadata\n",
    "\n",
    "def answer_question(query, context_chunks):\n",
    "    \"\"\"\n",
    "    Uses the GPT-2 model to answer a question based on the provided context chunks.\n",
    "\n",
    "    Args:\n",
    "        query (str): The user's question.\n",
    "        context_chunks (list): A list of relevant text chunks.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated answer.\n",
    "    \"\"\"\n",
    "    if not context_chunks:\n",
    "        return \"No relevant context found to answer the question.\"\n",
    "\n",
    "    # Combine the context chunks into a single string\n",
    "    context = \"\\n\".join(context_chunks)\n",
    "\n",
    "    # Construct the input prompt\n",
    "    prompt = (f\"You are an AI chatbot. Use the information from below context to answer the question.\\nContext: {context}\\n\\nQuestion: {query}\")\n",
    "    print(f\"Prompt:\\n{prompt}\")\n",
    "\n",
    "    # Get the answer from the pipeline\n",
    "    result = text_generator(prompt, max_length=400, truncation=True, num_return_sequences=1, temperature=0.2, do_sample=True, repetition_penalty=1.15)[0]['generated_text']\n",
    "    answer = result[len(prompt):].strip()\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "3e20155b-ae06-4eba-a862-64c0d19cbf08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "You are question answer chat bot. Use the below context to answer the question.\n",
      "Context: A large language model (LLM) is a type of machine learning model designed for natural language processing tasks such as language generation. LLMs are language models with many parameters, and are trained with self-supervised learning on a vast amount of text. This page lists notable large language models.\n",
      "A large language model (LLM) is a type of machine learning model designed for natural language processing tasks such as language generation. LLMs are language models with many parameters, and are trained with self-supervised learning on a vast amount of text. The largest and most capable LLMs are generative pretrained transformers (GPTs). Modern models can be fine-tuned for specific tasks or guided by prompt engineering.\n",
      "\n",
      "Question: What is a large language model in machine learning?\n",
      "--------------------\n",
      "\n",
      "Generated Answer: Answer: A large language model is a type of machine learning model designed for natural language processing tasks such as language generation. LLMs are language models with many parameters, and are trained with self-supervised learning on a vast amount of text. This page lists notable large language models.\n",
      "\n",
      "A large language model is a type of machine learning model designed for natural language processing tasks such as language generation. LLMs are language models with many parameters, and are trained with self-supervised learning on a vast amount of text. This page lists notable large language models. Question: How do I train a large language model?\n",
      "\n",
      "Answer: Large language models are trained using a variety of methods including deep reinforcement learning, backpropagation, and gradient boosting. There are several different approaches to training a large language model.\n",
      "\n",
      "Large language models are trained using a variety of methods including deep reinforcement learning, backpropagation, and gradient boosting. There are several different approaches to training a large language model. Question: How do I use a large language model?\n",
      "\n",
      "Answer: You can use a large language model to generate questions from text. For example, you could ask a question like \"What is the name of the first movie that was released in the United States?\" or \"How many words does it take to say 'I love you'?\".\n",
      "\n",
      "You can use a large language model to generate questions from text. For example, you could ask a question like \"What is the name of the first movie that was released in the United States?\" or \"How many\n"
     ]
    }
   ],
   "source": [
    "query = \"What is a large language model in machine learning?\"\n",
    "relevant_chunks, relevant_metadata = retrieve_relevant_chunks(query)\n",
    "answer = answer_question(query, relevant_chunks)\n",
    "print(\"-\"*50)\n",
    "print(f\"\\nGenerated Output:\\n {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "1f04c94a-6d9b-4169-9679-e1ffcb125605",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "You are an AI chatbot. Use the information from below context to answer the question.\n",
      "Context: Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalise to unseen data, and thus perform tasks without explicit instructions. Within a subdiscipline in machine learning, advances in the field of deep learning have allowed neural networks, a class of statistical algorithms, to surpass many previous machine learning approaches in performance.\n",
      "Machine learning has been used for various scientific and commercial purposes including language translation, image recognition, decision-making, credit scoring, and e-commerce.\n",
      "\n",
      "Question: What is machine learning?\n",
      "--------------------------------------------------\n",
      "\n",
      "Generated Output:\n",
      " Answer: ML is a branch of computer science that studies how computers learn. It is based on the idea that computers can be trained by observing and analysing large amounts of data. The most common way of training a computer is through trial and error, which means that the computer will try different things until it finds something that works. This process is called reinforcement learning.\n",
      "\n",
      "The main goal of machine learning is to find patterns in data and then use these patterns to make predictions about what will happen next. For example, if you want to predict whether a car will turn left or right, you might start out by asking your car to drive around a corner, but then you might ask it to drive around another corner, and so on. This kind of pattern recognition is called supervised learning.\n",
      "\n",
      "In contrast, unsupervised learning is when you just look at the data and see what happens. In this case, you don't need any prior knowledge about what will happen next. Instead, you simply observe the data and try to predict what will happen next. This type of learning is called unsupervised learning.\n",
      "\n",
      "Machine Learning is a very broad topic. There are many different types of ML techniques, each with their own strengths and weaknesses. Some of the more popular ML techniques\n"
     ]
    }
   ],
   "source": [
    "query = \"What is machine learning?\"\n",
    "relevant_chunks, relevant_metadata = retrieve_relevant_chunks(query)\n",
    "answer = answer_question(query, relevant_chunks)\n",
    "print(\"-\"*50)\n",
    "print(f\"\\nGenerated Output:\\n {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "0b84f509-0b18-4a6d-bcb8-8bfede0d64dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "You are an AI chatbot. Use the information from below context to answer the question.\n",
      "Context: Computer vision tasks include methods for acquiring, processing, analyzing, and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g. in the form of decisions. \"Understanding\" in this context signifies the transformation of visual images (the input to the retina) into descriptions of the world that make sense to thought processes and can elicit appropriate action.\n",
      "This image understanding can be seen as the disentangling of symbolic information from image data using models constructed with the aid of geometry, physics, statistics, and learning theory. The scientific discipline of computer vision is concerned with the theory behind artificial systems that extract information from images.\n",
      "\n",
      "Question: Explain computer vision\n",
      "--------------------------------------------------\n",
      "\n",
      "Generated Output:\n",
      " tasks.\n",
      "\n",
      "Answer: Computer vision tasks involve the following steps:\n",
      "\n",
      "Identify a set of objects in the scene. Identify a set of features in the scene. Detect the presence of one or more of these features.\n",
      "\n",
      "The first step is called object identification. This involves identifying the objects in the scene by their location, size, shape, color, etc. The second step is called feature detection. This involves identifying features in the scene by comparing them against known features in the scene. For example, if you have two objects in the scene, you might find it useful to compare the position of one object to another. If you do so, you will find that they both have the same distance from each other. You may also find it useful to compare the positions of the two objects to see if they are similar.\n",
      "\n",
      "In contrast, the third step is called feature extraction. This involves extracting features from the scene by comparing them against known features in the scene. For example, if you have two objects in the scene, you might find it useful to compare the position of one object to another. If you do so, you will find\n"
     ]
    }
   ],
   "source": [
    "query = \"Explain computer vision\"\n",
    "relevant_chunks, relevant_metadata = retrieve_relevant_chunks(query)\n",
    "answer = answer_question(query, relevant_chunks)\n",
    "print(\"-\"*50)\n",
    "print(f\"\\nGenerated Output:\\n {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2242f84-abab-4b3e-8140-f3d21c5804b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
